{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d290bad2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938616a2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca3d513",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12,8]\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f084ac5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4ea014",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the gaussian mixture models - EM notebook we have used the mixture models for _density estimation_ e.g. modeling the  distribution of features in each class. Another common application is _clustering_. Technically this looks the same as fitting the GMM to a single class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b587bd89",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's assume we have $K$ components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553ccb92",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\newcommand{\\b}[1]{\\mathbf{#1}}$\n",
    "$$p(\\b x|\\b \\theta) = \\sum_{k=1}^K\\pi_k p_\\mathcal{N}(\\b x|\\b\\mu_k,\\b\\Sigma_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3740970c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Generalizing the two component example from GMM-EM lecture we can introduce the latent variables $\\b z\\in \\{0,1\\}^K$ i.e. each $\\b z$ is a vector of length $K$ with all entries set to zero except entry $k$ set to one which indicates that this point belongs to $k$-th  component. This is called $1-K$ encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6b72fb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$z_i = \\{0,\\ldots,1,\\ldots,0\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af2cfc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The joint distribution in $\\b x$ and $\\b z$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40f2fd8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$p(\\b x, \\b z|\\b \\theta) = \\sum_{k=1}^K\\pi_k z_k p_\\mathcal{N}(\\b x|\\b\\mu_k,\\b\\Sigma_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafdbfcd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Please note that only one term in the sum on the right hand side is non-zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29688cf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "From Bayes theorem the probability that point $\\b x$ belongs to cluster $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc6633b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\gamma_k(\\b x)\\equiv p(z_k=1|\\b x, \\theta)=\n",
    "\\frac{ \\pi_k p_\\mathcal{N}(\\b x|\\b\\mu_k,\\b\\Sigma_k)}\n",
    "{\\sum_{k=1}^K\\pi_k  p_\\mathcal{N}(\\b x|\\b\\mu_k,\\b\\Sigma_k)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10d15de",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$\\gamma_k(\\b x)$ is called _responsibility_ of $\\b x$ for cluster $k$ and as a probability can take value from zero to one. This is called _soft clustering_. Of course we can use this number to define a _hard clustering_ by _e.g._ assigning points to the cluster with  highest responsibility. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a6a9b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Sex from heigh and weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda01cb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's apply this to  the sex from height&weight example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30220efc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from mchlearn.datasets import load_height_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2281fb08",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "hw_data = load_height_weight('../../Data/HeightWeight/weight-height.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34517c46",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee595b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "hw_train, hw_test = train_test_split(hw_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e316d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffacacb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "hw_gm = GaussianMixture(n_components=2, tol=1e-5, n_init=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ff86a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "hw_gm.fit(hw_train[['Height','Weight']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf6a46",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's look at the found clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af015789",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from mchlearn.plotting import confidence_ellipse\n",
    "plt.scatter(hw_train['Height'], hw_train['Weight'], alpha=0.2, color='grey');\n",
    "colors=['red', 'blue']\n",
    "for i in range(2):\n",
    "    confidence_ellipse(hw_gm.means_[i], hw_gm.covariances_[i], ax = plt.gca(), edgecolor=colors[i], label =f\"{hw_gm.weights_[i]:5.3f}\");\n",
    "    plt.scatter(*np.split(hw_gm.means_,2,1), facecolors=colors)\n",
    "plt.legend(title='$\\pi_k$');    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29acd607",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Looks very reasonable :) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb2cad8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Please recall that the because of the unidentifiability  of the parameters the particular assignment of labels to clusters is meaningless. That's a knowledge that we have to put in \"by hand\". Below I choose the women cluster as the one with smaller average height. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd9b91",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "female = np.argmin(hw_gm.means_[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e2dd72",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's compare the results to the separate fit to each cluster _i.e._ quadratic discriminant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2053b150",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "qda = QuadraticDiscriminantAnalysis(store_covariance=True)\n",
    "qda.fit(hw_train[['Height','Weight']], hw_train.Gender=='Female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97288a7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from mchlearn.plotting import confidence_ellipse\n",
    "plt.scatter(hw_train['Height'], hw_train['Weight'], alpha=0.2, color='grey');\n",
    "qda_colors = [colors[1-female],colors[female]]\n",
    "for i in range(2):\n",
    "    confidence_ellipse(hw_gm.means_[i], hw_gm.covariances_[i], ax = plt.gca(), edgecolor=colors[i], label =f\"{hw_gm.weights_[i]:5.3f}\");\n",
    "    plt.scatter(*np.split(hw_gm.means_,2,1), facecolors=colors)\n",
    "    confidence_ellipse(qda.means_[i], qda.covariance_[i], ax = plt.gca(), edgecolor=qda_colors[i], label =f\"{qda.priors_[i]:5.3f} QDA\", linestyle='--');\n",
    "    plt.scatter(*np.split(qda.means_,2,1), edgecolors=qda_colors, facecolors='none')\n",
    "plt.legend(title='$\\pi_k$');    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3e9a20",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As we can see the results are very similar. We can use those (EM fitted) cluster for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c660977",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "test_proba = hw_gm.predict_proba(hw_test[['Height','Weight']])\n",
    "confusion_matrix(hw_test.Gender=='Female', test_proba[:,female]>0.5, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b4495d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from mchlearn.plotting import roc_plot, add_roc_curve\n",
    "fig, ax = roc_plot()\n",
    "add_roc_curve(hw_test.Gender=='Female', test_proba[:,female], name='unsupervised', ax =ax);\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c8b168",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As you can see we are getting results similar to supervised learning. However in this analysis we have overlooked some very important issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99124068",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing  the number of components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3f5675",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "While technically the procedure of fitting is similar to supervised learning with gaussian mixture discriminative analysis (GMDA) the crucial difference lies in the interpretation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcafbda9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In GMDA the clusters are only a mean to better approximation of the class probability densities and their interpretation is irrelevant for the functioning of the classifier. We can choose the number of clusters that gives best classification results, because we have clear metrics to measure the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e965ea4e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In clustering we usually want to discover the structure of data and assign some interpretation to discovered clusters. But the  number of clusters is an input parameter. In the height-weight examples we knew that  this was two so the results were good. And we have used the real labels to check that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704fe55b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "But this is not a case in general. We could experiment with different number of clusters but we need the criteria for evaluating the quality of clustering in absence of real labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624479b3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc43c06",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As what we are doing is to find the distribution that  fits the data bests, we could use the log likelihood (LL) as the measure of the performance. And that's what EM lagorithm does: it tries to maximize the likelihood. It is customary to use instead the _negative log likelihood_  (NNL) which contrary to the name is positive :) The smaller NNL the better. For calculating the log likehood we can use the `score` method that returns the average LL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac20c39a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "-hw_gm.score(hw_train[['Height', 'Weight']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94b756c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "However we have already used the LL to fit on the same data. I hope that by now you know that you cannot do that and that this estimate is biased. We can try it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec9f3e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "-hw_gm.score(hw_test[['Height', 'Weight']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043c021d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "and indeed we get a bigger, hence worse value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fa7625",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's compare this to fit with three clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b51f209",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "hw_gm3 = GaussianMixture(n_components=3, tol=1e-5, n_init=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97764f15",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "hw_gm3.fit(hw_train[['Height', 'Weight']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972d699b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Looking at the fitted clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9ca076",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from mchlearn.plotting import confidence_ellipse\n",
    "plt.scatter(hw_train['Height'], hw_train['Weight'], alpha=0.2);\n",
    "cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "for i in range(3):\n",
    "    confidence_ellipse(hw_gm3.means_[i], hw_gm3.covariances_[i], ax = plt.gca(), edgecolor=cycle[i], label =f\"{hw_gm3.weights_[i]:5.3f}\");\n",
    "plt.legend(title='$\\pi_k$');    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec9fb72",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "we see that there is not a clear interpretation to be given to different clusters and that their weights are similar. Let's see how good is the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e70a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"2 clusters {-hw_gm.score(hw_test[['Height', 'Weight']]):6.4f}  3 clusters {-hw_gm3.score(hw_test[['Height', 'Weight']]):6.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf1a009",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We see that it is only marginally different then in case of two clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e53b4f2",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eedb27e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Actually we  should also not use the test set for choosing the number of components.  We should use a separate validation set. If we  do not have enough data we can recourse to cross valdation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8766f26b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cv2 = cross_validate(hw_gm, hw_train[['Height', 'Weight']])\n",
    "print(cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee93450c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cv3 = cross_validate(hw_gm3, hw_train[['Height', 'Weight']])\n",
    "print(cv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e1038c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"2 clusters {-cv2['test_score'].mean():6.4f}  3 clusters {-cv3['test_score'].mean():6.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958b152",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We see that the results are almost indistinguishable. But note that their are also different from the values obtained on the test set. That's because all of those are  statistical estimators so actually they are random variables themself :( The  estimate of variance of those estimators  is  difficult  and beyond the subject of this lecture. Interested readers may consult [No Unbiased Estimator of the Variance of K-Fold Cross-Validation](http://www.jmlr.org/papers/volume5/grandvalet04a/grandvalet04a.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
