{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8819c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc579f1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12,8]\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd4550",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d814c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Mixture models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7608c0c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "The  quadratic discriminant analysis made rather strong assumptions that that distributions of features in each class was multivariate normal. Obviously often this assumptions does not hold.  Consider the following synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fe0211",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "half_circles = np.loadtxt('half_circles.txt')\n",
    "hc_labels = half_circles[:,2].astype('int32')\n",
    "hc_data = half_circles[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c1d4d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "colors = np.asarray(['blue', 'red'])\n",
    "plt.scatter(half_circles[:,0], half_circles[:,1], s=30, alpha =0.5, \n",
    "            c=colors[half_circles[:,2].astype('int32')]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a1d10c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Clearly the distributions are not normal, the clusters are intertwined and althought they look well separated  we can expect the quadratic discriminant analysis to perform poorly on this data set. Let's  check it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677ba6ee",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "hc_train,hc_test, hc_lbl_train, hc_lbl_test = train_test_split(hc_data, hc_labels, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85596a4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "hc_qda = QuadraticDiscriminantAnalysis(store_covariance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88d342e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hc_qda.fit(hc_train, hc_lbl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a3c0d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qda_proba = hc_qda.predict_proba(hc_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b810db30",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00611a3",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_matrix(hc_lbl_test, qda_proba>0.5, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238afc83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mchlearn.plotting import roc_plot, add_roc_curve\n",
    "fig, ax = roc_plot()\n",
    "add_roc_curve(hc_lbl_test, qda_proba, 'qda', ax);\n",
    "ax.legend(title='AUC');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7453367",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "So while not totally useless the classifiers performance is not  exactly stellar. It's easy to understand why, when we look at confidence ellipses of fitted distributions and  resulting decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3677e81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mchlearn.plotting import confidence_ellipse, decision_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa23843",
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_red = np.asarray([[0., 0.,1.],[1.,0.,0.]]) \n",
    "decision_plot(half_circles[:,:2], half_circles[:,2].astype('int32'), blue_red, np.linspace(-3.,3.3,200), np.linspace(-1.5,1.5,200),hc_qda.predict_proba, np.argmax  )\n",
    "confidence_ellipse(hc_qda.means_[0], hc_qda.covariance_[0], ax = plt.gca(), edgecolor='blue')\n",
    "confidence_ellipse(hc_qda.means_[1], hc_qda.covariance_[1], ax = plt.gca(), edgecolor='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97720e13",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "The QDA classifier, constrained to quadratic curve, cannot reproduce the complex boundary between clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e65c4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Mixture of Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87d8aa",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "A simple generalisation of normal distribution is a _mixture of gaussians_ distribution which is a special case of _mixture model_. As the name implies those distribution consists of a _mixture_ of normal distributions. The resulting probability density function (pdf) is a  weighted sum of the pdfs of each mixture component. Modeling each class with the mixture of gaussians leads to so called _gaussian mixture discriminant analysis_. \n",
    "\n",
    "I will ilustrate the idea of mixture model on a simple mixture of two  univariate (one dimensional) gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3b6824",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### One dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878e04fb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "The pdf for a mixture of two  normal distributions is given by the formula "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e821d9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\\newcommand{\\nc}{\\mathcal{N}}$$\n",
    "$$p(x|\\pi, \\mu_0,\\sigma_0, \\mu_1, \\sigma_1)=\\pi \\cdot p_\\nc(x|\\mu_1,\\sigma_1) +  (1-\\pi) \\cdot p_\\nc(x|\\mu_0,\\sigma_0) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bd59e4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "where $p_\\nc(x|\\mu,\\sigma)$ denotes the pdf of normal distribution with mean $\\mu$ and standard deviation $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fb825",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mus  = np.asarray([0,1.5])\n",
    "stds = np.asarray([1,.5]) \n",
    "gstat = np.asarray([mus, stds]).T \n",
    "pi = 0.3\n",
    "#g = np.asarray([st.norm(*gstat[0]),st.norm(*gstat[1])])\n",
    "g = st.norm(loc=mus, scale=stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b42d8f7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def p_mix(x):\n",
    "    pdf = g.pdf(np.atleast_1d(x).reshape(-1,1))\n",
    "    return (1-pi)*pdf[:,0]+pi*pdf[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd69d704",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "xs = np.linspace(-5,5,500)\n",
    "pdf = g.pdf(xs.reshape(-1,1))\n",
    "plt.plot(xs, (1-pi)*pdf[:,0]+pi*pdf[:,1],label = '$\\pi \\cdot  p(x|\\mu_1,\\sigma_1) +  (1-\\pi) \\cdot p(x|\\mu_0,\\sigma_0)$')\n",
    "plt.plot(xs, (1-pi)*pdf[:,0], label = '$(1-\\pi) \\cdot  p(x|\\mu_0,\\sigma_0)$');\n",
    "plt.plot(xs, pi*pdf[:,1], label = '$\\pi \\cdot  p(x|\\mu_1,\\sigma_1)$');\n",
    "plt.legend(loc = 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb93712",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Sampling from a mixture distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422be9d8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "When sampling a mixture distribution we  do it in two steps: First we  select the component using the Bernoulli distribution (multinoulli in general when number of components is greater then two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f29ac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(12312)\n",
    "z  = st.bernoulli(p=pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a83d25",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "zs = z.rvs(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79bd5c7",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "and then we draw a sample from this component distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3591b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = st.norm.rvs(gstat[zs][:,0], gstat[zs][:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d732065",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is called _ancestral sampling_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597384b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(data, bins=64, density=True);\n",
    "plt.plot(xs, p_mix(xs));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a95e32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e52ce30",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "We will try to fit this distribution to data using MLE. Given the data $\\mathbf x$ the likelihood is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9069cf38",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\\newcommand{\\b}[1]{\\mathbf{#1}}$$\n",
    "$$p(\\b x|\\pi, \\mu_0,\\sigma_0, \\mu_1, \\sigma_1) = \\prod_{i=1}^N p(x_i|\\pi, \\mu_0\\sigma_0, \\mu_1, \\sigma_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7964bea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\\log p(\\b x|p, \\mu_0,\\sigma_0, \\mu_1, \\sigma_1) = \\sum_{i=1}^N \\log\\left(\n",
    "\\pi \\cdot p_\\nc(x_i|\\mu_1,\\sigma_1) +  (1-\\pi) \\cdot p_\\nc(x|\\mu_0,\\sigma_0)\n",
    "\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545acc82",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "This is similar to what we have encountered when fitting the quadratic discriminant model but with one crucial difference. Because we do not know which points belong to which cluster, the argument to the logarithm is a sum. As we cannot apply the logarithm separately to each term we end up with much more complicated expression. What's worse the resulting function is _not concave_ as a function of parameters $\\pi, \\mu_0,\\sigma_0, \\mu_1$ and $\\sigma_1$. That makes  finding the maximum estimate (MLE) much more difficult. We can of course use some universal maximum finding algorith like stochastic gradient descent but we have to ascertain that e.g. paramter $\\pi$ is  contained in the interval $[0,1)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14580aab",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Fortunatelly there exists an algorithm  that is aplicable to exactly such problems involving hidden variables, which I will describe below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6e196",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Latent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f81c5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Let's suppose we know to which component each data point belongs. We can encode this information into a variable $z$ that takes values $0$  and $1$. Then the _joint_ distribution of variables $x$ nad $z$ is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516db007",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "P(x,z|\\pi,\\mu_0,\\sigma_0, \\mu_1, \\sigma_1) = \n",
    "z \\cdot \\pi \\cdot p_\\nc(x|\\mu_1,\\sigma_1)+  (1-z)\\cdot(1-\\pi) \\cdot p_\\nc(x|\\mu_0,\\sigma_0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffd1272",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\displaystyle\\sum_{z=0}^1 P(x,z|\\pi,\\mu_0,\\sigma_0, \\mu_1, \\sigma_1) = \n",
    "\\pi \\cdot p_\\nc(x|\\mu_1,\\sigma_1)+(1-\\pi) \\cdot p_\\nc(x|\\mu_0,\\sigma_0) = P(x|\\pi,\\mu_0,\\sigma_0, \\mu_1, \\sigma_1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe31f0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "The log likelihood is "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2748510",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\\log p(\\b x|p, \\mu_0,\\sigma_0, \\mu_1, \\sigma_1) = \\sum_{i=1}^N \\log\\left(\n",
    "z\\cdot \\pi \\cdot p_\\nc(x_i|\\mu_1,\\sigma_1) + (1-z)\\cdot (1-\\pi) \\cdot p_\\nc(x|\\mu_0,\\sigma_0)\n",
    "\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e8eb24",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Because only one term under the  logarithm can be non-zero we can change the logarithm of sum into the sum of logarithms: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d82b2d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\\log\\left(\n",
    "z\\cdot \\pi \\cdot p_\\nc(x_i|\\mu_1,\\sigma_1) + (1-z)\\cdot (1-\\pi) \\cdot p_\\nc(x|\\mu_0,\\sigma_0)\n",
    "\\right) = \n",
    "z \\log\\left(\n",
    "\\cdot \\pi \\cdot p_\\nc(x_i|\\mu_1,\\sigma_1)\\right) + (1-z)\\log\\left(\\cdot (1-\\pi) \\cdot p_\\nc(x|\\mu_0,\\sigma_0)\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc82a2a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "so the final expression for log-likelihood is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198899d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\log P(\\b x,\\b z|\\pi,\\mu_0,\\sigma_0, \\mu_1, \\sigma_1) = \\sum_{i=1}^N \\left[ z_i \\log\\left(\n",
    "\\pi \\cdot p_\\nc(x_i|\\mu_1,\\sigma_1)\\right)+ (1-z_i)\\log \\left((1-p) \\cdot p_\\nc(x_i|\\mu_0,\\sigma_0)\n",
    "\\right)\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0032b57",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "The variables $z$ are called _latent_ or _unobserved_ variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e299895",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Expectation - Maximization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1f8495",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "The _marginal_ distribution of $z$ is "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e566b59",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$P(z =1)=\\int_x P(x,z=1|\\pi,\\mu_0,\\sigma_0, \\mu_1, \\sigma_1) =\n",
    "\\pi \\cdot  \\int_x p_\\nc(x|\\mu_1,\\sigma_1)=\\pi\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2705c865",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "We define conditional probability of $z$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef03faf0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\\gamma_\\theta(x)\\equiv P(z =1|x,\\theta),\\quad \\theta = \\{\\pi, \\mu_0,\\sigma_0, \\mu_1, \\sigma_1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7991474c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "where $\\theta$ denotes all the parameters $\\pi, \\mu_0,\\sigma_0, \\mu_1, \\sigma_1$. This is the probability that given $\\theta$ the point $x$ belongs to first cluster.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29dba88",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "It can be easilly calculated  using Bayes theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2646008",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$P(z =1|x,\\theta) = \n",
    "\\frac{p(x|z=1,\\theta)P(z=1)}\n",
    "{p(x|z=0, \\theta)P(z=0)+p(x|z=1,\\theta)P(z=1)}\n",
    "=\n",
    "\\frac{\\pi p_\\nc(x_i|\\mu_1,\\sigma_1)}{\\pi p_\\nc(x_i|\\mu_1, \\sigma_1)+ (1-\\pi) p_\\nc(x_i|\\mu_0,\\sigma_0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f5216",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Given $\\gamma_\\theta(x)$ we can calculate the __expected__ log likehood "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cb3004",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c619a20d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$Q(\\theta', \\theta) \\equiv E_{P(z|x, \\theta)}\\left[\\log P(x, z|\\theta')\\right]\\equiv \\sum_i \\left[\\gamma_\\theta(x_i) \\log P( x_i, z_i=1|\\theta')+(1-\\gamma_\\theta(x_i))\\log P( x_i, z_i=0|\\theta^')\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55ba37a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Please note that the expectation value was calculated  using the probability distribution for $\\b z$ with parameters $\\theta$. However in the  joint probability distribution being averaged $P(\\b x, z|\\theta)$ we have assumed a different set of parameters $\\b \\theta^\\prime$. The final expression is "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31f8e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\\sum_{i=1}^N \\left[\\gamma_\\theta(x_i)\\log p_\\nc(x_i|\\mu'_1,\\sigma'_1)+  (1-\\gamma_\\theta(x_i))\\log  p_\\nc(x_i|\\mu'_0,\\sigma'_0)\\right]\n",
    "+\n",
    "\\sum_{i=1}^N \\left[\\gamma_\\theta(x_i)\\log \\pi' +(1-\\gamma_\\theta(x_i))\\log (1-\\pi') \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082a7f7e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Now we can calculate the parameters $\\hat\\theta$ that __maximize__ this expectation value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe66553",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5245282c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\\newcommand{\\argmax}{\\operatorname{argmax}}$$\n",
    "$$\\hat\\theta = \\argmax_{\\theta'}Q(\\theta', \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b9c6cf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Let's calculate the $\\hat\\pi$. Differentiating the likelihood with respect to $\\pi'$ we obtain the equation for $\\hat\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867c7fc9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\\frac{1}{\\hat\\pi} \\sum_{i=1}^N\\gamma_\\theta(x_i)-\\frac{1}{1-\\hat\\pi}\\sum_{i=1}^N(1-\\gamma_\\theta(x_i))=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba45888",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "with solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e795a7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\\hat\\pi = \\frac{1}{N}\\sum_{i=1}^N\\gamma_\\theta(x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307eccb1",
   "metadata": {},
   "source": [
    "For $\\hat\\mu_1$ we have to differentiate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dcf5ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "$$\\sum_{i=1}^N \\gamma_\\theta(x_i)\\log p_\\nc(x_i|\\mu'_1,\\sigma'_1) = \n",
    "-\\log\\sigma'_1 \\sum_{i=1}^N \\gamma_\\theta(x_i) \n",
    "-\\sum_{i=1}^N \\gamma_\\theta(x_i)\\frac{1}{2\\sigma_1^2}(x_i-\\mu'_1)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9a9651",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "with respect to $\\mu'_1$ and we obtain equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ace5e5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\\frac{1}{\\sigma^2}\\sum_{i=1}^N \\gamma_\\theta(x_i)(x_i-\\hat\\mu_1) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37527ce",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "which is equivalent to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3005a313",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\\sum_{i=1}^N \\gamma_\\theta(x_i) x_i = \\hat\\mu_1\\sum_{i=1}^N \\gamma_\\theta(x_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a36d87",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "leading to "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4ec30e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\\hat\\mu_1 = \\frac{\\sum_{i=1}^N \\gamma_\\theta(x_i) x_i}{\\sum_{i=1}^N \\gamma_\\theta(x_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8e846f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "The rest of the parameters is calculated in the same way giving finally:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1119c1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\hat\\mu_0 = \\frac{\\sum_{i=1}^N  (1- \\gamma_\\theta(x_i)) x_i}{\\sum_{i=1}^N  (1-\\gamma_\\theta(x_i))}\n",
    "\\qquad\n",
    "\\hat\\mu_1 = \\frac{\\sum_{i=1}^N \\gamma_\\theta(x_i) x_i}{\\sum_{i=1}^N \\gamma_\\theta(x_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ecc3ba",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat\\sigma_0^2=\\frac{\\sum_{i=1}^N (1-\\gamma_\\theta(x_i)) (x_i-\\hat\\mu_0)^2}{\\sum_{i=1}^N  (1-\\gamma_\\theta(x_i))}\n",
    "\\qquad\n",
    "\\hat\\sigma_1^2=\\frac{\\sum_{i=1}^N \\gamma_\\theta(x_i) (x_i-\\hat\\mu_1)^2}{\\sum_{i=1}^N \\gamma_\\theta(x_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95755b4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Repeating this steps leads to the __Expectation-maximization__ (EM) algorithm:\n",
    "  1. Start by initialising the parameters $\\theta$\n",
    "  2. Calculate the $\\gamma_\\theta(x_i)$: that is the __expectation__ step\n",
    "  3. Use $\\gamma$ to calculate new parameters $\\hat\\theta$: that's the __maximization__ step\n",
    "  4. Set $\\theta = \\hat\\theta$\n",
    "  4. Repeat until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252889d1",
   "metadata": {},
   "source": [
    "[A. P. Dempster, N. M. Laird and D. B. Rubin, \"Maximum Likelihood from Incomplete Data via the EM Algorithm\" Journal of the Royal Statistical Society. Series B \n",
    "Vol. 39, No. 1 (1977), pp. 1-38](https://www.jstor.org/stable/2984875?seq=1#metadata_info_tab_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcd4cc5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Those steps for this simple two gaussians example are implemented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4228d5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expectation(x,theta):\n",
    "    pi, m1, s1, m2,s2 = theta\n",
    "    d2 = pi*st.norm(loc=m2, scale=s2).pdf(x)\n",
    "    d1 = (1-pi)*st.norm(loc=m1, scale=s1).pdf(x)\n",
    "    return d2/(d1+d2)\n",
    "\n",
    "def maximization(x,z):\n",
    "    N = len(z)\n",
    "    z_sum = z.sum()\n",
    "    m1 = np.sum((1-z)*x)/(N-z_sum)\n",
    "    m2 = np.sum(z*x)/z_sum\n",
    "    \n",
    "    s1 = np.sqrt(np.sum((1-z)*(x-m1)*(x-m1))/(N-z_sum))\n",
    "    s2 = np.sqrt(np.sum(z*(x-m2)*(x-m2))/z_sum)\n",
    "    \n",
    "    pi = z_sum/N\n",
    "    \n",
    "    return np.asarray([pi,m1,s1,m2,s2])\n",
    " \n",
    "def next_theta(x, theta):\n",
    "    z = expectation(x,theta)\n",
    "    return maximization(x,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4599a714",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Starting parameters\n",
    "# For mu I choose two data points at random\n",
    "start = np.random.choice(data,2, replace=False)\n",
    "# For sigma I use the std of the whole dataset\n",
    "theta = np.asarray([0.5, start[0], data.std(), start[1], data.std()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329696c2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Collect the results of 300 iterations\n",
    "ts = [theta]\n",
    "for i in range(300):\n",
    "    theta = next_theta(data,theta)\n",
    "    ts.append(theta)\n",
    "thetas = np.stack(ts, axis=0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b559bd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Below are plots showing the convergence of all parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a765f885",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdad34a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(len(thetas)), thetas[:,0], s=10, alpha=0.5, label='$\\pi$');\n",
    "plt.axhline(0.3);\n",
    "plt.axhline(0.7);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1ff22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916519d8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(len(thetas)), thetas[:,1], s=10, alpha=0.5, label = \"$\\mu_1$\", color = 'blue');\n",
    "plt.axhline(mus[0], color = 'blue')\n",
    "plt.scatter(range(len(thetas)), thetas[:,3], s=10, alpha=0.5, label = \"$\\mu_2$\", color = 'orange');\n",
    "plt.axhline(mus[1], color='orange');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ed2cf1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621c2a7",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(len(thetas)), thetas[:,2], s=10, alpha=0.5, label = \"$\\sigma_1$\", color = 'blue');\n",
    "plt.axhline(stds[0], color = 'blue')\n",
    "plt.scatter(range(len(thetas)), thetas[:,4], s=10, alpha=0.5, label = \"$\\sigma_2$\", color = 'orange');\n",
    "plt.axhline(stds[1], color='orange');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a146200",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "The algorithm is garantied (within numericall accuracy) never to decrease the likelihood, so let's check this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adcc41d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loglikehood(x,theta):\n",
    "    pi, m1, s1, m2,s2 = theta\n",
    "    d2 = pi * st.norm(loc=m2, scale=s2).pdf(x)\n",
    "    d1 = (1-pi) * st.norm(loc=m1, scale=s1).pdf(x)\n",
    "    return np.log(d1 + d2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055251eb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ll = [loglikehood(data,t) for t in thetas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98a9e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(ll)), ll, linewidth=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde627a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1,len(ll)), ll[1:], linewidth=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aeac34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(range(100,len(ll)), ll[100:], linewidth=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5434345d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "As you can see we gain most from the first step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c1d47",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Below is a  simple animation of the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfe1db",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from matplotlib.animation import FuncAnimation\n",
    "xs = np.linspace(-5,5,500)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim(0,0.4)\n",
    "l1, = plt.plot([-5,5],[0,0])\n",
    "l2, = plt.plot([-5,5],[0,0])\n",
    "l3, = plt.plot([-5,5],[0,0])\n",
    "plt.hist(data, bins=64,  density=True, histtype='step')\n",
    "\n",
    "\n",
    "def animate(theta):\n",
    "    pi, m1, s1, m2,s2 = theta\n",
    "    d2 = pi*st.norm(loc=m2, scale=s2).pdf(xs)\n",
    "    d1 = (1-pi)*st.norm(loc=m1, scale=s1).pdf(xs)\n",
    "    l1.set_data(xs,d1)\n",
    "    l2.set_data(xs,d2)\n",
    "    l3.set_data(xs,d1+d2)\n",
    "    \n",
    "    \n",
    "anim = FuncAnimation(fig, animate, thetas, repeat=False, interval=80)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d914d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Can take ~15 seconds to prepare\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e14642c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "## Notes on the convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c409ff2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Unidentifiability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76facdb9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$p(x|\\pi, \\mu_0,\\sigma_0, \\mu_1, \\sigma_1)=\\pi \\cdot p_\\nc(x|\\mu_1,\\sigma_1) +  (1-\\pi) \\cdot p_\\nc(x|\\mu_0,\\sigma_0),\\quad  \\pi\\leftrightarrow (1-\\pi), \\mu_0\\leftrightarrow \\mu_1, \\sigma_0\\leftrightarrow \\sigma_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe55aa44",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Looking at the model mixture distribution function it's easy to notice that it is independent  with respect to the permutations of the parameters i.e. if we exchange $\\pi\\leftrightarrow (1-\\pi)$, $\\mu_0\\leftrightarrow \\mu_1$ and $\\sigma_0\\leftrightarrow \\sigma_1$  the resulting pdf will be the same.  That's called _unidentifiability_. You can observe this on the convergence plots above. Different components are marked in different colors and as you can see the colors may not correspond to  the colors of the true values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b7952",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ac0bf0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "The EM algorithm is guaranteed not to decrease the likelihood at each step, so it will usually converge to some local maxima. One can start the algorithm with different initial parameters and choose the result with highest likelihood. \n",
    "\n",
    "Actually the fact that the algorithm will rather find local then global maximum is a good thing. For mixture of gaussian the global maximum is degenerate (!) and can be obtained as follows: pick one point $x_0$ in the data and consider $\\mu_0=x_0$ and $\\sigma_0\\rightarrow 0$. In this limit the contribution from this  single point to the likelihood will be infinite (why?) , while the other component will provide finite values for the rest of the data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2612ef4",
   "metadata": {},
   "source": [
    "$$p_\\nc(x|x,\\sigma_0)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\frac{(x-x)^2}{2\\sigma^2}} = \\frac{1}{\\sqrt{2\\pi}\\sigma} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1496760f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "This behaviour is illustrated below. This have been tuned to this particular dataset. I have fixed the seed when generating data, but I am not sure if it is portable across all the numpy versions, so you may need to experiment. \n",
    "\n",
    "Unfortunatelly the functions  defined above will fail because of the numerical instabilities. That is a common problem when dealing with probabilities that can have expotentially small values. The problem arises when we have small values in both numerator and denominator. The solution is to use logarithms and pull out the biggest term in the sum both in numerator and denominator. This is implemented in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1121e33",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = data[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98652ff1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expectation_stable(x,theta):\n",
    "    pi, m1, s1, m2,s2 = theta\n",
    "    d = st.norm([m1,m2],[s1,s2]).logpdf(x.reshape(-1,1))+np.log(np.asarray([[1-pi, pi]]))\n",
    "    d_max = np.max(d,1)\n",
    "    \n",
    "    return np.exp(d[:,1]-d_max)/np.sum(np.exp(d-d_max.reshape(-1,1)),1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b3405",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "#Some helper functions that will be used to break the iterations  when we reach a singularity\n",
    "def is_nan(t):\n",
    "    return np.any(np.isnan(t)) \n",
    "\n",
    "def valid_thetas(th):\n",
    "    not_nan =  ~np.any(np.isnan(thetas),1) \n",
    "    v_th = th[not_nan]\n",
    "    s_th = (v_th[:,2]>1.0e-10) & (v_th[:,4]>1.0e-10)\n",
    "    return v_th[s_th]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888ef083",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "def singular(theta):\n",
    "    for i in range(100):\n",
    "        z = expectation_stable(ds,theta)\n",
    "        theta = maximization(ds,z)\n",
    "        if is_nan(theta):\n",
    "            return i\n",
    "    return False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53194071",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    theta = [0.5, ds[3],  0.4, ds[i], 0.11]\n",
    "    l = singular(theta)\n",
    "    if l:\n",
    "        print(i,l, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f4244",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "theta = [0.5, ds[3],  0.4, ds[9], 0.121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e12804",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = [theta]\n",
    "for i in range(100):\n",
    "    z = expectation_stable(ds,theta)\n",
    "    theta = maximization(ds,z)\n",
    "    if  is_nan(theta):\n",
    "        break\n",
    "    res.append(theta)\n",
    "thetas = np.stack(res,0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411fdb6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(len(thetas)), thetas[:,2],s=5,label='$\\\\sigma_0$')\n",
    "plt.scatter(np.arange(len(thetas)), thetas[:,4],s=5,label='$\\\\sigma_1$');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad5a7f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If everything went all right with the example you should see on of the $\\sigma$'s go to zero on this plot.  This corresponds to one of the distributions collapsing onto one point as illustrated in the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d47618",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "def plot(theta,ax):\n",
    "    xs = np.linspace(-4,4,5000)\n",
    "    ax.set_ylim(0,2.75)\n",
    "    pi, m1, s1, m2,s2 = theta\n",
    "    d2 = pi*st.norm(loc=m2, scale=s2).pdf(xs)\n",
    "    d1 = (1-pi)*st.norm(loc=m1, scale=s1).pdf(xs)\n",
    "    ax.plot(xs,d1,c='red')\n",
    "    ax.plot(xs,d2,c='blue')\n",
    "    #plt.plot(xs,d1+d2,c='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd141e4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(19,6))\n",
    "plot(thetas[5],ax[0])\n",
    "plot(thetas[6],ax[1])\n",
    "plot(thetas[7],ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea195f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from matplotlib.animation import FuncAnimation\n",
    "x_extent = (-4,4)\n",
    "xs = np.linspace(*x_extent,5000)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim(0,3)\n",
    "ax.set_xlim(*x_extent)\n",
    "l1, = plt.plot(x_extent,[0,0], color='blue')\n",
    "l2, = plt.plot(x_extent,[0,0], color='red')\n",
    "l3, = plt.plot(x_extent,[0,0], color='green')\n",
    "plt.hist(ds, bins=16,  density=True, histtype='step', color='green', range=x_extent)\n",
    "#plt.stem(ds, np.ones_like(ds), markerfmt='none')\n",
    "\n",
    "\n",
    "def animate(theta):\n",
    "    pi, m1, s1, m2,s2 = theta\n",
    "    d2 = pi*st.norm(loc=m2, scale=s2).pdf(xs)\n",
    "    d1 = (1-pi)*st.norm(loc=m1, scale=s1).pdf(xs)\n",
    "    l1.set_data(xs,d1)\n",
    "    l2.set_data(xs,d2)\n",
    "    #l3.set_data(xs,d1+d2)\n",
    "    \n",
    "anim = FuncAnimation(fig, animate, valid_thetas(thetas), repeat=False, interval=1000)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe4954",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60910f45",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "This behaviour will be much more pronounced in higher dimensions. That is because with same number of points, the points in higher dimensions will be on average more distanced from each other.  Thus it is easier to \"isolate\" one point which leads to the described singularity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc35dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "###  Maximal a posteriori (MAP)  estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61037af6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "This problem can be avoided by  using the full Bayesian approach. To this end we add prior for $\\pi$ and  for parameters $\\mu_k$ and $\\sigma_k$. For the EM algorithm this amounts to changing the expected log likelihood to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fd6f0a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\begin{split}\n",
    "\\sum_{i=1}^N &\\left[\\gamma_\\theta(x_i)\\log p_\\nc(x_i|\\mu'_1,\\sigma'_1) +  (1-\\gamma_\\theta(x_i))\\log  p_\\nc(x_i|\\mu'_0,\\sigma'_0)\\right] \n",
    "+\\sum_{i=1}^N \\left[\\gamma_\\theta(x_i)\\log \\pi' +(1-\\gamma_\\theta(x_i))\\log (1-\\pi') \\right]\\\\\n",
    "&+\\log P(\\pi) +\\log P(1-\\pi)+\\log P(\\mu_0,\\sigma_0) +\\log P(\\mu_1, \\sigma_1)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7330ac9f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For $\\pi$ we chose  [Beta](https://en.wikipedia.org/wiki/Beta_distribution) $(\\alpha, \\beta)$ distribution prior and  [Normal Inverse Gamma](https://en.wikipedia.org/wiki/Normal-inverse-gamma_distribution) $(m_k,\\lambda_k, \\alpha_k,\\beta_k)\\;k=0,1$ prior for parameters $\\mu_k$ and $\\sigma_k$.  Fitting with those priors is described in detail in the normal distribution notebook. The result is "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4baa8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "$$\\hat\\pi = \\frac{\\sum_{i=1}^N\\gamma_\\theta(x_i)+\\alpha-1}{N+\\alpha+\\beta-2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab322b3d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\hat\\mu_0 = \\frac{\\sum_{i=1}^N  (1- \\gamma_\\theta(x_i)) x_i +\\lambda_0 m_0}{\\sum_{i=1}^N  (1-\\gamma_\\theta(x_i))+\\lambda_0}\n",
    "\\qquad\n",
    "\\hat\\mu_1 = \\frac{\\sum_{i=1}^N \\gamma_\\theta(x_i) x_i+\\lambda_1 m_1}{\\sum_{i=1}^N \\gamma_\\theta(x_i)+\\lambda_1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76fe895",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\hat\\sigma_0=\\frac{\\sum_{i=1}^N (1-\\gamma_\\theta(x_i)) (x_i-\\hat\\mu_0)^2+2\\beta_0 \n",
    "+\\lambda_0(\\hat\\mu_0-m)^2}\n",
    "{\\sum_{i=1}^N  (1-\\gamma_\\theta(x_i))+2\\alpha_0 + 3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1821efdf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\hat\\sigma_1=\\frac{\\sum_{i=1}^N \\gamma_\\theta(x_i) (x_i-\\hat\\mu_1)^2+\n",
    "2\\beta_1 +\\lambda_1(\\hat\\mu_1-m_1)^2}\n",
    "{\\sum_{i=1}^N \\gamma_\\theta(x_i)+2\\alpha_1 + 3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152e789",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "That introduces 10 (!) more hyperparameters: $\\alpha$  and $\\beta$ for Beta distribution and $m_k,\\lambda_k, \\alpha_k,\\beta_k$ for two Normal Inverse  Gamma priors on $\\mu_k$ and $\\sigma_k$ ($k=0,1$). \n",
    "We can simplify things assuming a symmetric prior on $\\pi$ with $\\alpha=\\beta$ leading to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e66296",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\\hat\\pi = \\frac{\\sum_{i=1}^N\\gamma_\\theta(x_i)+\\alpha-1}{N+2\\alpha-2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1ccaf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "$\\alpha$ smaller then one will tend to concentrate the probability mass on one of the components and higher values of  $\\alpha$ will favor more uniform  distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d308d85a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Setting $\\lambda_k = 0$ effectively removes the prior on the $\\mu_k$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21b33ea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\hat\\mu_0 = \\frac{\\sum_{i=1}^N  (1- \\gamma_\\theta(x_i)) x_i}{\\sum_{i=1}^N  (1-\\gamma_\\theta(x_i))}\n",
    "\\qquad\n",
    "\\hat\\mu_1 = \\frac{\\sum_{i=1}^N \\gamma_\\theta(x_i) x_i}{\\sum_{i=1}^N \\gamma_\\theta(x_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f2671",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "and finally we will set $\\alpha_k$ to $3/2$ resulting in: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54316a55",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\hat\\sigma_0=\\frac{\\sum_{i=1}^N (1-\\gamma_\\theta(x_i)) (x_i-\\hat\\mu_0)^2+2\\beta_0}\n",
    "{\\sum_{i=1}^N  (1-\\gamma_\\theta(x_i))+2\\alpha_0 + 3}\n",
    "\\quad\n",
    "\\hat\\sigma_1=\\frac{\\sum_{i=1}^N \\gamma_\\theta(x_i) (x_i-\\hat\\mu_1)^2+\n",
    "2\\beta_1}\n",
    "{\\sum_{i=1}^N \\gamma_\\theta(x_i)+2\\alpha_1 + 3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14756943",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "That leaves us  with only three additional parameters. Those formulas are implemented in functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b9baf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MAP(x,z, a,b0, b1):\n",
    "    N = len(z)\n",
    "    z_sum = z.sum()\n",
    "    m1 = np.sum((1-z)*x)/(N-z_sum)\n",
    "    m2 = np.sum(z*x)/z_sum\n",
    "    \n",
    "    s1 = np.sqrt((np.sum((1-z)*(x-m1)*(x-m1))+2*b0)/(N-z_sum+6))\n",
    "    s2 = np.sqrt((np.sum(z*(x-m2)*(x-m2))+2*b1)/(z_sum+6))\n",
    "    \n",
    "    pi = (z_sum+a-1)/(N+2*a-2)\n",
    "    \n",
    "    return np.asarray([pi,m1,s1,m2,s2])\n",
    " \n",
    "def next_theta_MAP(x, theta,a,b0,b1):\n",
    "    z = expectation_stable(x,theta)\n",
    "    return MAP(x,z,a,b0,b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d3d9ab",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "I leave it to you to check  they behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5446c4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Gaussian  Mixture Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a0ea5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Half circles revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f9139",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Armed with the EM algorithm we now come back to the half circles example. However we will not write our own EM algorithm for multivariate normal distribution, but we will use an implementation from scikit-learn library provided by the `GaussianMixture` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab20a0aa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff5577",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We fill fit each class separatelly to a two Gaussian mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec835dfe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hc0_cmp = GaussianMixture(n_components=2, max_iter=100, tol=0.0001) \n",
    "hc1_cmp = GaussianMixture(n_components=2, max_iter=100, tol=0.0001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba1f11",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hc0 = hc_train[hc_lbl_train==0]\n",
    "hc1 = hc_train[hc_lbl_train==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c28238",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hc0_cmp.fit(hc0)\n",
    "hc1_cmp.fit(hc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5533549a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "The fitted parameters are accesible as attributes of the `GaussianMixture` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f1536",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(hc0_cmp.weights_)  #pi\n",
    "print(hc0_cmp.means_)    #mu\n",
    "print(hc0_cmp.covariances_) #Sigma (covariance) matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194c7ce4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Now we will use those fitted mixtures to construct a classifier. In each class $k=0,1$ the class conditial probability is given by the mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774091ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "$$p(x|c=k)=\\pi_{k} p_\\mathcal{N}(x|\\b\\mu_k,\\b\\Sigma_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3332ff46",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "The class probability given $\\b x$  is calculated from Bayes theorem:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639de1db",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$p(c=k|\\b x)=\\frac{p(x|c=k) P(c=k)}{\\sum_{k}p(x|c=k) P(c=k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2592d028",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "This is implemented in the functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860735f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_pdf(cmp):\n",
    "    \"\"\"\n",
    "    Takes a GaussianMixture object and returns corresponding\n",
    "    probability distribution function\n",
    "    \"\"\"\n",
    "    n_cmp = cmp.n_components\n",
    "    dists = [st.multivariate_normal(cmp.means_[i], cmp.covariances_[i]) for i in range(n_cmp)]\n",
    "    def pdf(x):\n",
    "        p = 0.0\n",
    "        for i in range(n_cmp):\n",
    "            p+= cmp.weights_[i]*dists[i].pdf(x)\n",
    "        return p\n",
    "    \n",
    "    return pdf\n",
    "    \n",
    "    \n",
    "def make_predict_proba(cmp0, cmp1, pi0=0.5, pi1=.5):\n",
    "    \"\"\"\n",
    "    Takes two GaussianMixture object and corresponding priors and returns \n",
    "    pdf for conditional probability P(c=1|x)\n",
    "    \"\"\"\n",
    "    pdf0 = make_pdf(cmp0)\n",
    "    pdf1 = make_pdf(cmp1)\n",
    "    def p(x):\n",
    "        p0=pi0*pdf0(x)\n",
    "        p1=pi1*pdf1(x)\n",
    "        return p1/(p1+p0)    \n",
    "        \n",
    "    return p\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb4690",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mgd_predict_proba = make_predict_proba(hc0_cmp, hc1_cmp, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0917b862",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mgd_proba = mgd_predict_proba(hc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36958cf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_matrix(hc_lbl_test, mgd_proba>0.5, normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7057e65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mchlearn.plotting import roc_plot, add_roc_curve\n",
    "fig, ax = roc_plot()\n",
    "add_roc_curve(hc_lbl_test, qda_proba, 'qda', ax);\n",
    "add_roc_curve(hc_lbl_test, mgd_proba, 'mga', ax);\n",
    "ax.legend(title='AUC');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdce633",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "We see a dramatic improvement in the quality of the classifier.  Which is explainable by looking at the fitted components and the new decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ea3e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "blue_red = np.asarray([[0., 0.,1.],[1.,0.,0.]]) \n",
    "decision_plot(half_circles[:,:2], half_circles[:,2].astype('int32'), blue_red, np.linspace(-2.5,2.5,200), np.linspace(-1.5,1.5,200),\n",
    "              mgd_predict_proba, lambda x: 0 if x<0.5 else 1  , ax = ax)\n",
    "confidence_ellipse(hc0_cmp.means_[0], hc0_cmp.covariances_[0], ax = ax, edgecolor='blue')\n",
    "confidence_ellipse(hc0_cmp.means_[1], hc0_cmp.covariances_[1], ax = ax, edgecolor='blue')\n",
    "confidence_ellipse(hc1_cmp.means_[0], hc1_cmp.covariances_[0], ax = ax, edgecolor='red')\n",
    "confidence_ellipse(hc1_cmp.means_[1], hc1_cmp.covariances_[1], ax = ax, edgecolor='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0793e211",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Choosing the number of components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba29abca",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "skip"
    ]
   },
   "source": [
    "In the example above we have somewhat arbitrarly choosen the number of gaussian components to be two for each class. This was justified by the shape of the distribution. However in general case we would be unable to look at the high dimensional data. It's obvious that choosing a bigger number of componets will better model the _training_ data. But at certain point it will overfit and start degrading the performace of the classifier on the test set. As with other hyperparameters, one way of  searching for best number of components is to use grid search with cross validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ca1e7b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "Unfortunatelly we cannot use the scikit-learn functionality  described in text analysis notebook because our classifier does not conform to the expected interface.  We can either make our own cross validation search using e.g. scikit-learn [`KFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) or `StratifiedKFold` classes or write our own classifier in a way conformant to the scikit-learn interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9376014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(nc0, nc1,X,y, X_valid, y_valid):\n",
    "\n",
    "    hc0_cmp = GaussianMixture(n_components=nc0, max_iter=100, tol=0.0001) \n",
    "    hc1_cmp = GaussianMixture(n_components=nc1, max_iter=100, tol=0.0001) \n",
    "\n",
    "    hc0 = X[y==0]\n",
    "    hc1 = X[y==1]\n",
    "\n",
    "    hc0_cmp.fit(hc0)\n",
    "    hc1_cmp.fit(hc1)\n",
    "    \n",
    "    gmda =  make_predict_proba(hc0_cmp, hc1_cmp, 0.5, 0.5)\n",
    "    proba = gmda(X_valid)\n",
    "    \n",
    "    return f1_score(y_valid, proba>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0965519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(2,2,hc_train, hc_lbl_train, hc_test, hc_lbl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a0974",
   "metadata": {},
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeeedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088ff097",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = KFold(5,shuffle=True, random_state=67544)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67dc67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder.get_n_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e180f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=0\n",
    "for train_i, test_i in folder.split(hc_train, hc_lbl_train):\n",
    "    f1+=evaluate(2,2,hc_train[train_i], hc_lbl_train[train_i], hc_train[test_i], hc_lbl_train[test_i])\n",
    "print(f1/folder.get_n_splits())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
