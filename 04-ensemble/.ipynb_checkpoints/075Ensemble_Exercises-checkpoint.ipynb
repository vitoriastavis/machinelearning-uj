{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods. Exercises\n",
    "\n",
    "\n",
    "In this section we have only two exercise:\n",
    "\n",
    "1. Find the best three classifier in the stacking method using the classifiers from scikit-learn package.\n",
    "\n",
    "2. Build arcing arc-x4 method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r data_set\n",
    "%store -r labels\n",
    "%store -r test_data_set\n",
    "%store -r test_labels\n",
    "%store -r unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Find the best three classifier in the stacking method\n",
    "\n",
    "Please use the following classifiers:\n",
    "\n",
    "* Linear regression,\n",
    "* Nearest Neighbors,\n",
    "* Linear SVM,\n",
    "* Decision Tree,\n",
    "* Naive Bayes,\n",
    "* QDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifiers():\n",
    "    # save each label and model in a dictionary\n",
    "    clf_dict = {}\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(data_set, labels)\n",
    "    clf_dict['lr'] = lr\n",
    "    \n",
    "    neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "    neigh.fit(data_set, labels)\n",
    "    clf_dict['neigh'] = neigh\n",
    "    \n",
    "    svc = SVC(kernel='linear') \n",
    "    svc.fit(data_set, labels)\n",
    "    clf_dict['svc'] = svc\n",
    "    \n",
    "    dt = DecisionTreeClassifier(random_state=0)\n",
    "    dt.fit(data_set, labels)\n",
    "    clf_dict['dt'] = dt\n",
    "    \n",
    "    nb = GaussianNB()\n",
    "    nb.fit(data_set, labels)\n",
    "    clf_dict['nb'] = nb\n",
    "    \n",
    "    scores = {}\n",
    "    for clf in clf_dict:\n",
    "        model = clf_dict[clf]\n",
    "        scores[clf] = model.score(data_set, labels)\n",
    "       \n",
    "    # 3 best scores\n",
    "    highest = dict(sorted(scores.items(), key=lambda item: item[1])[:3])\n",
    "    print('3 highest scores are: ', highest)\n",
    "    \n",
    "    classifiers = []\n",
    "    for clf in clf_dict:\n",
    "        if clf in highest:\n",
    "            classifiers.append(clf_dict[clf])\n",
    "    \n",
    "    return classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stacked_classifier(classifiers):\n",
    "    output = []\n",
    "    test_set = []\n",
    "    for clf in classifiers:\n",
    "        y_pred = clf.predict(data_set)\n",
    "        output.append(y_pred)\n",
    "        test_set.append(clf.predict(test_data_set))\n",
    "            \n",
    "    output = np.array(output).reshape((130,3))\n",
    "    \n",
    "    # stacked classifier part:\n",
    "    stacked_classifier = QuadraticDiscriminantAnalysis()\n",
    "    stacked_classifier.fit(output.reshape((130,3)), labels.reshape((130,)))\n",
    "    \n",
    "    test_set = np.array(test_set).reshape((len(test_set[0]),3))\n",
    "    \n",
    "    predicted = stacked_classifier.predict(test_set)\n",
    "    \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 highest scores are:  {'lr': 0.9287417709377138, 'nb': 0.9692307692307692, 'neigh': 0.9769230769230769}\n",
      "0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vsa/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/vsa/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "/home/vsa/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "classifiers = build_classifiers()\n",
    "predicted = build_stacked_classifier(classifiers)\n",
    "accuracy = accuracy_score(test_labels, predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: \n",
    "\n",
    "Use the boosting method and change the code to fullfilt the following requirements:\n",
    "\n",
    "* the weights should be calculated as:\n",
    "$w_{n}^{(t+1)}=\\frac{1+ I(y_{n}\\neq h_{t}(x_{n})}{\\sum_{i=1}^{N}1+I(y_{n}\\neq h_{t}(x_{n})}$,\n",
    "* the prediction is done with a voting method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# prepare data set\n",
    "\n",
    "def generate_data(sample_number, feature_number, label_number):\n",
    "    data_set = np.random.random_sample((sample_number, feature_number))\n",
    "    labels = np.random.choice(label_number, sample_number)\n",
    "    return data_set, labels\n",
    "\n",
    "labels = 2\n",
    "dimension = 2\n",
    "test_set_size = 1000\n",
    "train_set_size = 5000\n",
    "train_set, train_labels = generate_data(train_set_size, dimension, labels)\n",
    "test_set, test_labels = generate_data(test_set_size, dimension, labels)\n",
    "\n",
    "# init weights\n",
    "number_of_iterations = 10\n",
    "weights = np.ones((test_set_size,)) / test_set_size\n",
    "\n",
    "def train_model(classifier, weights):\n",
    "    return classifier.fit(X=test_set, y=test_labels, sample_weight=weights)\n",
    "\n",
    "def calculate_error(model):\n",
    "    predicted = model.predict(test_set)\n",
    "    I=calculate_accuracy_vector(predicted, test_labels)\n",
    "    Z=np.sum(I)\n",
    "    return (1+Z)/1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the two functions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_new_weights(model, count_missclf):\n",
    "    # prediction \n",
    "    y_pred = model.predict(test_set)\n",
    "    \n",
    "    weights = []    \n",
    "    for i in range(test_set_size):\n",
    "        if y_pred[i] != test_labels[i]:\n",
    "            # increment counter if it was missclassified\n",
    "            count_missclf[i] += 1\n",
    "        \n",
    "        # numerator\n",
    "        num = 1 + count_missclf[i]\n",
    "        \n",
    "        # denominator\n",
    "        denum = 0\n",
    "        for i in range(test_set_size):\n",
    "            # sum the same thing many times. seems wrong but I didn't find a well-explained formula\n",
    "            denum += 1 + count_missclf[i]\n",
    "        \n",
    "        weight = num/denum\n",
    "        weights.append(weight)\n",
    "    \n",
    "    return weights, count_missclf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the classifier with the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0009184423218221896, 0.0011019283746556473, 0.0012853470437017994, 0.0011017260374586854, 0.0011017260374586854, 0.0011015237745548008, 0.0011013215859030838, 0.0011013215859030838, 0.0011011194714626537, 0.0011011194714626537, 0.0011011194714626537, 0.0011009174311926607, 0.0009174311926605505, 0.001100715465052284, 0.0011005135730007337, 0.0011005135730007337, 0.0011005135730007337, 0.000917094644167278, 0.0011005135730007337, 0.0011003117549972493, 0.0011003117549972493, 0.0009169264624977076, 0.0011001100110011, 0.0010999083409715857, 0.0010997067448680353, 0.0010997067448680353, 0.0010997067448680353, 0.0010997067448680353, 0.0010995052226498076, 0.0010993037742762918, 0.001099102399706906, 0.001098901098901099, 0.0009157509157509158, 0.001098901098901099, 0.001098901098901099, 0.001098901098901099, 0.0009157509157509158, 0.0010986998718183483, 0.0010984987184181618, 0.001098297638660077, 0.0010980966325036604, 0.0010978956999085087, 0.0010978956999085087, 0.0010978956999085087, 0.0010976948408342481, 0.0010976948408342481, 0.0009147457006952067, 0.0010976948408342481, 0.0009147457006952067, 0.0010976948408342481, 0.001097494055240534, 0.0010972933430870519, 0.0010970927043335162, 0.001279707495429616, 0.0010966916468652897, 0.0010966916468652897, 0.0010966916468652897, 0.0010964912280701754, 0.0010962908825141605, 0.0010960906101571063, 0.0010960906101571063, 0.0010960906101571063, 0.0010960906101571063, 0.0010960906101571063, 0.0010960906101571063, 0.0010960906101571063, 0.001095890410958904, 0.001095890410958904, 0.001095690284879474, 0.001095690284879474, 0.0010954902318787657, 0.0010954902318787657, 0.0010954902318787657, 0.001095290251916758, 0.0009127418765972983, 0.001095290251916758, 0.0010950903449534588, 0.0010948905109489052, 0.0010946907498631637, 0.0010944910616563297, 0.0010944910616563297, 0.0010942914462885281, 0.0009119095385737735, 0.0010942914462885281, 0.0010940919037199124, 0.0010940919037199124, 0.0010940919037199124, 0.0010938924339106654, 0.0010938924339106654, 0.0012759752096244987, 0.001093693036820999, 0.0010934937124111536, 0.0010932944606413995, 0.0010932944606413995, 0.0010932944606413995, 0.001093095281472035, 0.001093095281472035, 0.001092896174863388, 0.001092697140775815, 0.0010924981791697013, 0.0010922992900054614, 0.0010922992900054614, 0.0012741172187841281, 0.001091901728844404, 0.001091703056768559, 0.001091703056768559, 0.001091703056768559, 0.0010915044569765327, 0.0010915044569765327, 0.0010915044569765327, 0.0009095870474804438, 0.0010913059294288831, 0.0010913059294288831, 0.0010913059294288831, 0.0010913059294288831, 0.0009094216078574026, 0.0010913059294288831, 0.0012729587197672304, 0.0010911074740861974, 0.0010911074740861974, 0.0009090909090909091, 0.001090909090909091, 0.0010907107798582077, 0.001272264631043257, 0.0010903143739778303, 0.0010901162790697674, 0.0010899182561307902, 0.0010899182561307902, 0.0010897203051216855, 0.0010895224260032686, 0.0010895224260032686, 0.0010895224260032686, 0.0010893246187363835, 0.0010891268832819024, 0.001088929219600726, 0.001088929219600726, 0.001088929219600726, 0.0010887316276537834, 0.0010885341074020319, 0.00090711175616836, 0.00090711175616836, 0.0010885341074020319, 0.0010883366588064574, 0.0010883366588064574, 0.001088139281828074, 0.0009067827348567283, 0.0009067827348567283, 0.0010879419764279238, 0.0010877447425670776, 0.001087547580206634, 0.0010873504893077202, 0.0009061254077564335, 0.0010871534698314912, 0.0010869565217391304, 0.0010867596449918493, 0.0010867596449918493, 0.0010867596449918493, 0.0010865628395508873, 0.0010865628395508873, 0.0009054690329590728, 0.0009054690329590728, 0.0010863661053775121, 0.0010863661053775121, 0.0010861694424330196, 0.0010859728506787331, 0.0010857763300760044, 0.0010857763300760044, 0.001085579880586213, 0.001085579880586213, 0.001085579880586213, 0.001085579880586213, 0.001085383502170767, 0.0009044862518089725, 0.0010851871947911015, 0.00108499095840868, 0.0009041591320072332, 0.0010847947929849937, 0.0010845986984815619, 0.0010844026748599313, 0.0009036688957166094, 0.0010844026748599313, 0.0010844026748599313, 0.0010842067220816769, 0.001084010840108401, 0.001083815028901734, 0.001083815028901734, 0.001083619288423334, 0.0010834236186348862, 0.0012637660227477884, 0.0010832280194981044, 0.0010830324909747292, 0.0010828370330265296, 0.0010826416456153013, 0.0010826416456153013, 0.0010826416456153013, 0.0010824463287028685, 0.0010824463287028685, 0.0009020386072523904, 0.0012626262626262627, 0.0010822510822510823, 0.0010822510822510823, 0.0010822510822510823, 0.0010822510822510823, 0.0010822510822510823, 0.0010820559062218215, 0.0010818608005769925, 0.001081665765278529, 0.001081665765278529, 0.0010814708002883922, 0.0009012256669069935, 0.0010814708002883922, 0.0010814708002883922, 0.001081275905568571, 0.001081275905568571, 0.001081275905568571, 0.0009010632546404757, 0.001081081081081081, 0.0009009009009009009, 0.001081081081081081, 0.001081081081081081, 0.001081081081081081, 0.001081081081081081, 0.0010808863267879661, 0.0009007386056566384, 0.0010808863267879661, 0.001080691642651297, 0.001080691642651297, 0.0012605798667386999, 0.0010803024846957148, 0.00108010801080108, 0.00108010801080108, 0.00108010801080108, 0.0008999280057595393, 0.001079719272989023, 0.0010795250089960418, 0.0010795250089960418, 0.0010795250089960418, 0.0010793308148947653, 0.0010793308148947653, 0.001079136690647482, 0.0008992805755395684, 0.0008992805755395684, 0.0008992805755395684, 0.001079136690647482, 0.001258766408919259, 0.001078942636216508, 0.0010787486515641855, 0.0010785547366528852, 0.0010783608914450035, 0.0010783608914450035, 0.0010783608914450035, 0.001078167115902965, 0.0008984725965858042, 0.001078167115902965, 0.001078167115902965, 0.0010779734099892202, 0.0010777797736662474, 0.0010777797736662474, 0.0010775862068965517, 0.0010775862068965517, 0.000897827258035554, 0.0010771992818671453, 0.0012565069107880094, 0.0010770059235325794, 0.0010768126346015793, 0.0010766194150367845, 0.0010766194150367845, 0.001076426264800861, 0.0010762331838565023, 0.0008968609865470852, 0.0010762331838565023, 0.0008968609865470852, 0.0008968609865470852, 0.001255380200860832, 0.0010758472296933835, 0.0010756543564001434, 0.0008963786303334528, 0.0008963786303334528, 0.0010754615522495072, 0.001075268817204301, 0.0008960573476702509, 0.0010750761512273786, 0.0010750761512273786, 0.0010750761512273786, 0.0010748835542816195, 0.0010746910263299302, 0.0010744985673352436, 0.0010743061772605193, 0.0010743061772605193, 0.0012531328320802004, 0.0010741138560687433, 0.0010739216037229282, 0.0010737294201861132, 0.0010735373054213634, 0.0008946144211844695, 0.0010735373054213634, 0.0010735373054213634, 0.0012522361359570662, 0.001073345259391771, 0.001073345259391771, 0.0010731532820604543, 0.001072961373390558, 0.001072769533345253, 0.001072769533345253, 0.001072769533345253, 0.0010725777618877368, 0.000893814801573114, 0.0010725777618877368, 0.0010725777618877368, 0.0010723860589812334, 0.0010723860589812334, 0.0012508934953538242, 0.0010720028586742897, 0.0010720028586742897, 0.0008933357155619082, 0.0010718113612004287, 0.0010718113612004287, 0.0010718113612004287, 0.0010716199321307376, 0.0010716199321307376, 0.0010714285714285715, 0.0010712372790573112, 0.0010710460549803642, 0.0010710460549803642, 0.0010708548991611637, 0.0010708548991611637, 0.0010706638115631692, 0.0008922198429693076, 0.0010704727921498661, 0.0010702818408847663, 0.0010702818408847663, 0.0008917424647761727, 0.0008917424647761727, 0.001070090957731407, 0.0010699001426533524, 0.0010697093956141914, 0.0010697093956141914, 0.0010695187165775401, 0.0010693281055070398, 0.0010693281055070398, 0.0010691375623663579, 0.0012471049349723855, 0.0010689470871191875, 0.0010687566797292483, 0.0010685663401602849, 0.0010685663401602849, 0.0010683760683760685, 0.0010681858643403952, 0.001245995016019936, 0.0010678056593699946, 0.0010678056593699946, 0.0012455516014234875, 0.0010676156583629894, 0.0010674257249599715, 0.0010674257249599715, 0.0010674257249599715, 0.0010674257249599715, 0.000889521437466643, 0.0010674257249599715, 0.0010672358591248667, 0.0010672358591248667, 0.0010670460608216254, 0.0010668563300142249, 0.0010666666666666667, 0.001066477070742979, 0.001066477070742979, 0.0010662875422072153, 0.000888572951839346, 0.0010662875422072153, 0.0010660980810234541, 0.0008884150675195451, 0.0010660980810234541, 0.0008884150675195451, 0.0010660980810234541, 0.0008884150675195451, 0.0010659086871558003, 0.0008882572392965003, 0.0010659086871558003, 0.0008880994671403197, 0.0010657193605683837, 0.0010657193605683837, 0.0010655301012253596, 0.001065340909090909, 0.001065340909090909, 0.0010651517841292384, 0.0008876264867743654, 0.0010651517841292384, 0.0010651517841292384, 0.0010651517841292384, 0.0010649627263045794, 0.0010649627263045794, 0.0010649627263045794, 0.001064773735581189, 0.0010645848119233499, 0.0010645848119233499, 0.0010643959552953698, 0.0010643959552953698, 0.001064207165661582, 0.001064018442986345, 0.0010638297872340426, 0.0010636411983690835, 0.0010636411983690835, 0.0010634526763559022, 0.0010634526763559022, 0.001063264221158958, 0.0010630758327427356, 0.0010630758327427356, 0.0012400354295837024, 0.0010628875110717448, 0.0010628875110717448, 0.0010626992561105207, 0.0010626992561105207, 0.0010625110678236232, 0.0010623229461756375, 0.0008852691218130312, 0.0010623229461756375, 0.0012391573729863693, 0.0010621348911311736, 0.0010619469026548673, 0.0010617589807113785, 0.0010617589807113785, 0.0010615711252653928, 0.0008846426043878273, 0.0010613833362816203, 0.0010611956137247967, 0.0008843296781039972, 0.0010611956137247967, 0.0008841732979664014, 0.0008841732979664014, 0.0008841732979664014, 0.0010610079575596816, 0.0010610079575596816, 0.0010610079575596816, 0.0008841732979664014, 0.0010608203677510608, 0.0010606328442637441, 0.0010604453870625664, 0.0012369676621311187, 0.0010602579961123874, 0.0010602579961123874, 0.0010600706713780918, 0.0010598834128245894, 0.0010598834128245894, 0.0010596962204168139, 0.0010595090941197245, 0.0010595090941197245, 0.0010595090941197245, 0.0010595090941197245, 0.001059322033898305, 0.001059322033898305, 0.001059135039717564, 0.001059135039717564, 0.001059135039717564, 0.0010589481115425344, 0.0010589481115425344, 0.0010589481115425344, 0.0010587612493382743, 0.0010587612493382743, 0.0010587612493382743, 0.0010587612493382743, 0.0010587612493382743, 0.0012350035285815103, 0.0010583877227024166, 0.0008819897689186806, 0.0010582010582010583, 0.0010582010582010583, 0.0010580144595309468, 0.0010578279266572638, 0.0008815232722143865, 0.0010576414595452142, 0.0010576414595452142, 0.0010576414595452142, 0.0010576414595452142, 0.0010576414595452142, 0.0010576414595452142, 0.0010574550581600281, 0.0010574550581600281, 0.0010572687224669603, 0.0010572687224669603, 0.0010572687224669603, 0.0010570824524312897, 0.0010568962480183195, 0.0010567101091933778, 0.0010567101091933778, 0.0010567101091933778, 0.0010567101091933778, 0.0010565240359218173, 0.0010565240359218173, 0.0010565240359218173, 0.0010565240359218173, 0.001056338028169014, 0.001056338028169014, 0.001056338028169014, 0.0010561520859003696, 0.0010561520859003696, 0.0010561520859003696, 0.0010561520859003696, 0.0010559662090813093, 0.0007039774727208729, 0.0010557803976772831, 0.0010557803976772831, 0.0008798169980644026, 0.0010557803976772831, 0.0010557803976772831, 0.0008798169980644026, 0.001055594651653765, 0.001055594651653765, 0.001055594651653765, 0.0008796622097114708, 0.001055594651653765, 0.001055594651653765, 0.0008796622097114708, 0.001055594651653765, 0.0010554089709762533, 0.0010554089709762533, 0.0010554089709762533, 0.0010554089709762533, 0.001055223355610271, 0.0012308774397749253, 0.0010548523206751054, 0.001054666901037089, 0.001054481546572935, 0.0010542962572482868, 0.0010541110330288123, 0.0010539258738802037, 0.001053740779768177, 0.0010535557506584723, 0.001053370786516854, 0.001053370786516854, 0.001053370786516854, 0.00105318588730911, 0.001053001053001053, 0.0008775008775008775, 0.001053001053001053, 0.001053001053001053, 0.001052816283558519, 0.0008773469029654326, 0.0010526315789473684, 0.0008771929824561404, 0.0010524469391334855, 0.001052262364082778, 0.001052262364082778, 0.0010520778537611783, 0.0010518934081346423, 0.0010518934081346423, 0.0010518934081346423, 0.0010517090271691498, 0.0010515247108307045, 0.0010513404590853338, 0.001051156271899089, 0.0008759635599159075, 0.001051156271899089, 0.0008759635599159075, 0.0008759635599159075, 0.001051156271899089, 0.0010509721492380452, 0.0010507880910683013, 0.0010506040973559797, 0.0010506040973559797, 0.0010506040973559797, 0.0010504201680672268, 0.001050236303168213, 0.0010500525026251313, 0.0010500525026251313, 0.0010498687664041995, 0.0010496850944716584, 0.0010496850944716584, 0.0010496850944716584, 0.001049501486793773, 0.001049317943336831, 0.0010491344640671447, 0.0010491344640671447, 0.001048951048951049, 0.001048951048951049, 0.0012237762237762239, 0.001048767697954903, 0.001048767697954903, 0.001048767697954903, 0.001048767697954903, 0.0008739730816290859, 0.0010485844110450892, 0.0010485844110450892, 0.0010484011881880134, 0.0010482180293501049, 0.001222707423580786, 0.0010480349344978166, 0.001047851903597625, 0.001047851903597625, 0.0010476689366160294, 0.001047486033519553, 0.0010473031942747426, 0.0010471204188481676, 0.0010469377072064212, 0.0010469377072064212, 0.0010469377072064212, 0.0010469377072064212, 0.00104675505931612, 0.0010465724751439038, 0.0010463899546564353, 0.001046207497820401, 0.0010460251046025104, 0.0010460251046025104, 0.0010460251046025104, 0.0008716875871687587, 0.001220149904131079, 0.0010456605088881143, 0.0010454783063251437, 0.0010454783063251437, 0.0010452961672473868, 0.0010452961672473868, 0.0010451140916216688, 0.0010451140916216688, 0.0010451140916216688, 0.000870928409684724, 0.0010449320794148381, 0.0010447501305937664, 0.0008706251088281386, 0.0010447501305937664, 0.0008706251088281386, 0.0010445682451253482, 0.0008704735376044568, 0.0012184508268059183, 0.0012182387747998607, 0.0010442046641141664, 0.0010442046641141664, 0.0010440229685053071, 0.0010440229685053071, 0.0012178148921363954, 0.0010436597669159853, 0.0008697164724299878, 0.0010434782608695651, 0.0008695652173913044, 0.0010432968179447052, 0.001043115438108484, 0.0010429341213280027, 0.0010427528675703858, 0.0010427528675703858, 0.0010425716768027802, 0.0010423905489923557, 0.0010422094841063053, 0.0008685079034219212, 0.0010422094841063053, 0.0010422094841063053, 0.0008685079034219212, 0.0010420284821118443, 0.0008683570684265369, 0.0010418475429762111, 0.0010418475429762111, 0.0008682062858135093, 0.0010416666666666667, 0.0010416666666666667, 0.0010414858531504947, 0.0010414858531504947, 0.0010413051023950017, 0.001041124414367517, 0.001041124414367517, 0.001041124414367517, 0.001041124414367517, 0.001041124414367517, 0.001041124414367517, 0.001041124414367517, 0.001041124414367517, 0.001040943789035392, 0.0010407632263660018, 0.0010407632263660018, 0.001040582726326743, 0.001040582726326743, 0.001040582726326743, 0.0010404022888850356, 0.0008670019074041963, 0.0010402219140083217, 0.0010402219140083217, 0.0010402219140083217, 0.0010402219140083217, 0.0010400416016640667, 0.0010400416016640667, 0.0010400416016640667, 0.0012131715771230502, 0.0010396811644429042, 0.0010396811644429042, 0.0010396811644429042, 0.0010395010395010396, 0.0012125411397886714, 0.0010391409767925182, 0.001038961038961039, 0.001038781163434903, 0.0012117015752120478, 0.0010386013501817552, 0.0012114918656974732, 0.001038241910365115, 0.0010380622837370243, 0.0010380622837370243, 0.0010378827192527244, 0.0010378827192527244, 0.0010378827192527244, 0.0010378827192527244, 0.0010378827192527244, 0.0010377032168799724, 0.0008647526807333103, 0.0010375237765865469, 0.0012102351313969572, 0.0008644536652835408, 0.001037344398340249, 0.0012100259291270526, 0.0010369858278603526, 0.0012096077414895455, 0.0010368066355624676, 0.0010366275051831375, 0.0008638562543192813, 0.0008638562543192813, 0.0010364484366902746, 0.0010362694300518134, 0.0008635578583765112, 0.0010362694300518134, 0.0010362694300518134, 0.0010362694300518134, 0.0008635578583765112, 0.0010360904852357105, 0.0010360904852357105, 0.0010360904852357105, 0.0010359116022099447, 0.0010357327809425167, 0.0010357327809425167, 0.0010355540214014498, 0.0008629616845012082, 0.0010353753235547887, 0.0010351966873706005, 0.0010351966873706005, 0.0010350181128169743, 0.0010348395998620215, 0.0010346611484738748, 0.0010346611484738748, 0.0010344827586206897, 0.001034304430270643, 0.001206480523957256, 0.0012062726176115801, 0.0008616232982939859, 0.0010337698139214334, 0.0010337698139214334, 0.0010335917312661498, 0.0010334137099552187, 0.0012054417082831067, 0.0010332357499569485, 0.0010332357499569485, 0.0010332357499569485, 0.0010330578512396695, 0.0010330578512396695, 0.0010330578512396695, 0.0010330578512396695, 0.0010330578512396695, 0.0010328800137717334, 0.0010327022375215145, 0.0010327022375215145, 0.0010327022375215145, 0.0010325245224574084, 0.0010325245224574084, 0.0010323468685478321, 0.0010323468685478321, 0.0012041974883880955, 0.0010319917440660474, 0.0010319917440660474, 0.0010318142734307824, 0.0010316368638239339, 0.0010316368638239339, 0.0010316368638239339, 0.0010316368638239339, 0.0010314595152140279, 0.0012031625988312134, 0.0010311050008592541, 0.0008592541673827118, 0.0008592541673827118, 0.0008592541673827118, 0.0010311050008592541, 0.0010311050008592541, 0.0010309278350515464, 0.0010309278350515464, 0.0010307507301151005, 0.0010305736860185502, 0.0010303967027305513, 0.0010303967027305513, 0.0010302197802197802, 0.0008583690987124463, 0.0010300429184549357, 0.0008583690987124463, 0.0010298661174047373, 0.0006865774116031583, 0.0010298661174047373, 0.0010296893770379268, 0.0010296893770379268, 0.001029512697323267, 0.001029512697323267, 0.001029512697323267, 0.001029336078229542, 0.0008577800651912849, 0.001029336078229542, 0.0008576329331046312, 0.0010291595197255575, 0.0010289830217801406, 0.00102880658436214, 0.001028630207440425, 0.001028630207440425, 0.0010284538909838875, 0.0010284538909838875, 0.0010282776349614395, 0.0010282776349614395, 0.0010282776349614395, 0.001028101439342015, 0.001027925304094569, 0.001027925304094569, 0.0010277492291880781, 0.0010277492291880781, 0.0010275732145915396, 0.0010273972602739725, 0.0010273972602739725, 0.0010273972602739725, 0.0010273972602739725, 0.001027221366204417, 0.0010270455323519343, 0.0010268697586856067, 0.001026694045174538, 0.001026518391787853, 0.0010263427984946972, 0.0010263427984946972, 0.001026167265264238, 0.001026167265264238, 0.0010259917920656635, 0.0010259917920656635, 0.0010259917920656635, 0.0010258163788681826, 0.0008548469823901521, 0.0010258163788681826, 0.0010258163788681826, 0.0010256410256410256, 0.0010256410256410256, 0.0010256410256410256, 0.0010254657323534439, 0.0010252904989747095, 0.0010252904989747095, 0.0010251153254741158, 0.0010249402118209772, 0.0010249402118209772, 0.0010247651579846286, 0.0010247651579846286, 0.0010247651579846286, 0.0010245901639344263, 0.0010245901639344263, 0.0010245901639344263, 0.0008538251366120218, 0.0010245901639344263, 0.0010244152296397474, 0.0010242403550699897, 0.0010240655401945725, 0.0008533879501621437, 0.0008533879501621437, 0.0010238907849829352, 0.0008532423208191126, 0.0010237160894045385, 0.0010237160894045385, 0.0010235414534288639, 0.0010235414534288639, 0.0010233668770254136, 0.0010231923601637107, 0.0011935208866155158, 0.0008523695874531196, 0.0010226691665246293, 0.0010226691665246293, 0.0010224948875255625, 0.0010223206679161698, 0.0010223206679161698, 0.0010223206679161698, 0.0010223206679161698, 0.0010223206679161698, 0.0010221465076660989, 0.001021972406745018, 0.0011920980926430518, 0.0008513536523071684, 0.0010214504596527069, 0.0010214504596527069, 0.0010214504596527069, 0.0010214504596527069, 0.0010212765957446808, 0.0010211027910142954, 0.0010211027910142954, 0.0010211027910142954, 0.0010211027910142954, 0.0008509189925119128, 0.0010211027910142954, 0.0010211027910142954, 0.0010211027910142954, 0.0010211027910142954, 0.0010211027910142954, 0.0010211027910142954, 0.0008509189925119128, 0.0010211027910142954, 0.0010209290454313426, 0.0010207553589656345, 0.0008506294658046955, 0.0010207553589656345, 0.0010205817315870045, 0.0010204081632653062, 0.0010204081632653062, 0.0010204081632653062, 0.0010204081632653062, 0.0010204081632653062, 0.0010202346539704133, 0.0010202346539704133, 0.0008501955449753444, 0.0010200612036722204, 0.0010198878123406426, 0.0010197144799456153, 0.0010195412064570944, 0.0010193679918450561, 0.0010193679918450561, 0.0010191948360794973, 0.0010190217391304348, 0.0010190217391304348, 0.0010190217391304348, 0.0010188487009679063, 0.0010186757215619694, 0.001188253267696486, 0.0010185028008827025, 0.0010183299389002036, 0.001018157135584592, 0.001018157135584592, 0.001018157135584592, 0.001018157135584592, 0.0010179843909060061, 0.0010178117048346056, 0.0010176390773405698, 0.0010176390773405698, 0.0010176390773405698, 0.0010176390773405698, 0.0011870442597931151, 0.0010174665083940986, 0.001017293997965412, 0.0010171215460247499, 0.0010171215460247499, 0.001016949152542373, 0.0011862396204033216, 0.0011860386309725517, 0.0008471704506946798, 0.0010166045408336157, 0.0010166045408336157, 0.0010166045408336157, 0.001016432322547857, 0.0010162601626016261, 0.0010160880609652837, 0.0010160880609652837, 0.001015916017609211, 0.001015916017609211, 0.001015744032503809, 0.001015744032503809, 0.0008464533604198408, 0.0008464533604198408, 0.001184834123222749, 0.001015400236926722, 0.0010152284263959391, 0.0010152284263959391, 0.0010152284263959391, 0.0010152284263959391, 0.0010152284263959391, 0.0010152284263959391, 0.0010152284263959391, 0.0010152284263959391, 0.0010152284263959391, 0.0010150566739976314, 0.0010150566739976314, 0.0010148849797023004, 0.0008455944529003889, 0.0011836320595197836, 0.0008454514710855597, 0.0010145417653026716, 0.0010143702451394759, 0.0010141987829614604, 0.0010141987829614604, 0.001014027378739226, 0.001014027378739226, 0.001014027378739226, 0.0011828320378506252, 0.0008448800270361609, 0.001013856032443393, 0.001013856032443393, 0.001013856032443393, 0.0010136847440446021, 0.0010135135135135136, 0.0010133423408208073, 0.0010133423408208073, 0.0010131712259371835, 0.0010130001688333614, 0.0010130001688333614, 0.0010130001688333614, 0.0010130001688333614, 0.0010130001688333614, 0.0008440243079000675, 0.0010126582278481013, 0.0010126582278481013, 0.0010124873439082012, 0.0010124873439082012, 0.001181035937236376, 0.0008435970980259828]\n"
     ]
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier(max_depth=1, random_state=1)\n",
    "classifier.fit(X=train_set, y=train_labels)\n",
    "alphas = []\n",
    "classifiers = []\n",
    "count_missclf = [0]*test_set_size\n",
    "for iteration in range(number_of_iterations):\n",
    "    model = train_model(classifier, weights)\n",
    "    weights, count_missclf = set_new_weights(model, count_missclf)\n",
    "    classifiers.append(model)\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the validation data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_x, validate_label = generate_data(5, dimension, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why does train_model function use  the test dataset? in prediction should we use test as well? what about validation dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the prediction code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(x):  \n",
    "    prediction = []\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        voting = []\n",
    "        preds = []\n",
    "        \n",
    "        for clf in classifiers:\n",
    "            # predict with the model\n",
    "            y_pred = clf.predict(x[i].reshape(1, -1))\n",
    "            # append to vector of predictions for this x[i]\n",
    "            preds.append(y_pred[0])\n",
    "            \n",
    "            # if the prediction is right\n",
    "            if y_pred[0] == validate_label[i]:\n",
    "                # true to the voting array\n",
    "                voting.append(1) \n",
    "            else:\n",
    "                # else, false\n",
    "                voting.append(0)\n",
    "                \n",
    "        # the prediction is the argmax of the voting array\n",
    "        prediction.append(preds[np.argmax(voting)])\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "prediction = get_prediction(validate_x)\n",
    "\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
