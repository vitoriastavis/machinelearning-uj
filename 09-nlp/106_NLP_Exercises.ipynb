{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NLP Exercises\n",
    "\n",
    "We have five exercises in this section. The exercises are:\n",
    "1. Build your own tokenizer, where you need to implement two functions to implement a tokenizer based on regular expression.\n",
    "2. Get tags from Trump speech.\n",
    "3. Get the nouns in the last 10 sentences from Trump's speech and find the nouns divided by sentencens. Use SpaCy.\n",
    "4. Build your own Bag Of Words implementation using tokenizer created before.\n",
    "5. Build a 5-gram model and clean up the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1. Build your own tokenizer\n",
    "\n",
    "Build two different tokenizers:\n",
    "- ``tokenize_sentence``: function tokenizing text into sentences,\n",
    "- ``tokenize_word``: function tokenizing text into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentences:\n",
      "['Here we go again.', 'I was supposed to add this text later.', \"Well, it's 10.\", 'p.', 'm.', \"here, and I'm actually having fun making this course.\", ':o I hope you are getting along fine with this presentation, I really did try.', 'And one last sentence, just so you can test your tokenizers better.']\n",
      "Tokenized words:\n",
      "['Here', 'we', 'go', 'again', 'I', 'was', 'supposed', 'to', 'add', 'this', 'text', 'later', 'Well', \"it's\", '10', '.p.m', 'here', 'and', \"I'm\", 'actually', 'having', 'fun', 'making', 'this', 'course', 'o', 'I', 'hope', 'you', 'are', 'getting', 'along', 'fine', 'with', 'this', 'presentation', 'I', 'really', 'did', 'try', 'And', 'one', 'last', 'sentence', 'just', 'so', 'you', 'can', 'test', 'your', 'tokenizers', 'better']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def tokenize_words(text: str) -> list:\n",
    "\n",
    "    words = re.findall(r\"\\b\\w+(?:'\\w+)?|\\b[\\w.]+\\b\", text)\n",
    "    \n",
    "    # filter stop words\n",
    "    #words_filtered = [w for w in words if w not in nltk.corpus.stopwords.words(\"english\") and w.isalpha()]\n",
    "    \n",
    "    return words\n",
    "\n",
    "def tokenize_sentence(text: str) -> list:\n",
    "    \n",
    "    # this one doesn't split p.m. but it also doesn't split the emoji\n",
    "    #sentences = re.split(r'(?<!\\b\\w)\\.(?!\\d|\\s[a-z]|p\\.m\\.)(?=\\s[A-Z])', text)\n",
    "  \n",
    "    sentences = re.findall(r\"(?<!\\b\\w)\\.(?!\\d|\\s[a-z]|p\\.m\\.)(?=\\s[A-Z]|:\\w)|(?<=:\\w)\\.|[^\\.\\?!]+(?:[\\.\\?!]|$)\", text)\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "text = \"Here we go again. I was supposed to add this text later.\\\n",
    " Well, it's 10.p.m. here, and I'm actually having fun making this course. :o\\\n",
    " I hope you are getting along fine with this presentation, I really did try.\\\n",
    " And one last sentence, just so you can test your tokenizers better.\"\n",
    "\n",
    "print(\"Tokenized sentences:\")\n",
    "print(tokenize_sentence(text))\n",
    "\n",
    "print(\"Tokenized words:\")\n",
    "print(tokenize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 2. Get tags from Trump speech using NLTK\n",
    "\n",
    "You should use the ``trump.txt`` file, read it and find the tags for each word. Use NLTK for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"trump.txt\", \"r\",encoding=\"utf-8\") \n",
    "trump = file.read()\n",
    "words = nltk.word_tokenize(trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_d = {key: value for key, value in tags}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 3. Get the nouns in the last 10 sentences from Trump's speech and find the nouns divided by sentencens. Use SpaCy.\n",
    "\n",
    "Please use Python list features to get the last 10 sentences and display nouns from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(doc.sents)[-n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [] \n",
    "\n",
    "for sentence in sentences:\n",
    "    nouns_sentence = []\n",
    "    \n",
    "    for word in sentence:     \n",
    "        # if the word is in the tag dictionary\n",
    "        if word.text in tags_d.keys():\n",
    "            # check if it is a noun\n",
    "            if tags_d[word.text] == 'NN' or tags_d[word.text] == 'NNP' or tags_d[word.text] == 'NNS':\n",
    "                nouns_sentence.append(word.text)\n",
    "                \n",
    "    nouns.append(nouns_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['vision', 'years', 'freedom', 'tonight', 'chapter', 'greatness'],\n",
       " ['time', 'thinking'],\n",
       " ['time', 'fights'],\n",
       " ['courage',\n",
       "  'share',\n",
       "  'dreams',\n",
       "  'hearts',\n",
       "  'bravery',\n",
       "  'hopes',\n",
       "  'souls',\n",
       "  'confidence',\n",
       "  'hopes',\n",
       "  'dreams',\n",
       "  'action'],\n",
       " ['America',\n",
       "  'aspirations',\n",
       "  'fears',\n",
       "  'future',\n",
       "  'failures',\n",
       "  'past',\n",
       "  'vision',\n",
       "  'doubts'],\n",
       " ['citizens', 'renewal', 'spirit'],\n",
       " ['Members', 'Congress', 'things', 'country'],\n",
       " ['everyone', 'tonight', 'moment'],\n",
       " ['Believe', 'yourselves', 'future', 'America'],\n",
       " ['Thank', 'God', 'God', 'United']]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. Build your own Bag Of Words implementation using tokenizer created before \n",
    "\n",
    "You need to implement following methods:\n",
    "\n",
    "- ``fit_transform`` - gets a list of strings and returns matrix with it's BoW representation\n",
    "- ``get_features_names`` - returns list of words corresponding to columns in BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 2 0 0 0 0 0 1 1 1 1 1 1 2 1]]\n",
      "['Bag', 'Of', 'Words', 'is', 'based', 'on', 'counting', 'words', 'occurences', 'throughout', 'multiple', 'documents', 'This', 'the', 'third', 'document', 'As', 'you', 'can', 'see', 'most', 'of', 'occur', 'only', 'once', 'gives', 'us', 'a', 'pretty', 'sparse', 'matrix', 'below', 'Really']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "class BagOfWords:\n",
    "    \"\"\"Basic BoW implementation.\"\"\"\n",
    "       \n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.nlp = spacy.load('en_core_web_sm')  \n",
    "        self.bow_list = []\n",
    "    \n",
    "    def fit_transform(self, corpus: list):\n",
    "        # tokenize and count words \n",
    "        for text in corpus:\n",
    "            tokens = tokenize_words(text)\n",
    "            for token in tokens:\n",
    "                if token in self.vocab:\n",
    "                    self.vocab[token] += 1\n",
    "                else:\n",
    "                    self.vocab[token] = 1\n",
    "\n",
    "        # create a matrix representation of the bow\n",
    "        bow_matrix = np.zeros((len(corpus), len(self.vocab)), dtype=int)\n",
    "        \n",
    "        for i, text in enumerate(corpus):\n",
    "            tokens = tokenize_words(text)\n",
    "            for token in tokens:\n",
    "                if token in self.vocab:\n",
    "                    j = list(self.vocab.keys()).index(token)\n",
    "                    bow_matrix[i, j] += 1\n",
    "\n",
    "        self.bow_list.append(bow_matrix)\n",
    "        \n",
    "        return bow_matrix      \n",
    "\n",
    "    def get_feature_names(self) -> list:\n",
    "        return list(self.vocab.keys())\n",
    "\n",
    "corpus = [\n",
    "     'Bag Of Words is based on counting',\n",
    "     'words occurences throughout multiple documents.',\n",
    "     'This is the third document.',\n",
    "     'As you can see most of the words occur only once.',\n",
    "     'This gives us a pretty sparse matrix, see below. Really, see below',\n",
    "]    \n",
    "    \n",
    "vectorizer = BagOfWords()\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X)\n",
    "\n",
    "features=vectorizer.get_feature_names()\n",
    "print(features)\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5. Build a 5-gram model and clean up the results.\n",
    "\n",
    "There are three tasks to do:\n",
    "1. Use 5-gram model instead of 3.\n",
    "2. Change to capital letter each first letter of a sentence.\n",
    "3. Remove the whitespace between the last word in a sentence and . ! or ?.\n",
    "\n",
    "Hint: for 2. and 3. implement a function called ``clean_generated()`` that takes the generated text and fix both issues at once. It could be easier to fix the text after it's generated rather then doing some changes in the while loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *\n",
    "\n",
    "wall_street = text7.tokens\n",
    "tokens = wall_street\n",
    "\n",
    "def cleanup():\n",
    "    compiled_pattern = re.compile(\"^[a-zA-Z0-9.!?]\")\n",
    "    clean = list(filter(compiled_pattern.match,tokens))\n",
    "    return clean\n",
    "\n",
    "tokens = cleanup()\n",
    "\n",
    "def build_ngrams():\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-N+1):\n",
    "        ngrams.append(tokens[i:i+N])\n",
    "    return ngrams\n",
    "\n",
    "def ngram_freqs(ngrams):\n",
    "    counts = {}\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        token_seq  = SEP.join(ngram[:-1])\n",
    "        last_token = ngram[-1]\n",
    "\n",
    "        if token_seq not in counts:\n",
    "            counts[token_seq] = {}\n",
    "\n",
    "        if last_token not in counts[token_seq]:\n",
    "            counts[token_seq][last_token] = 0\n",
    "\n",
    "        counts[token_seq][last_token] += 1;\n",
    "\n",
    "    return counts\n",
    "\n",
    "def next_word(text, N, counts):\n",
    "\n",
    "    token_seq = SEP.join(text.split()[-(N-1):]);\n",
    "    \n",
    "    if token_seq not in counts:\n",
    "        return None\n",
    "    \n",
    "    choices = counts[token_seq].items();\n",
    "\n",
    "    total = sum(weight for choice, weight in choices)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for choice, weight in choices:\n",
    "        upto += weight;\n",
    "        if upto > r: return choice\n",
    "    assert False # should not reach here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difficult or impossible if a person lacks adequate shelter . The interactions between health and homelessness are complex defying sweeping generalizations as to cause or effect . If we look to the future preventing homelessness is an important objective . This will require us to develop a much more sophisticated understanding of the dynamics of homelessness than we currently possess an understanding that can be developed only through careful study and research . William R.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def clean_generated(generated):\n",
    "    \n",
    "    # change to capital letter each first letter of a sentence\n",
    "    # and remove whitespaces before punctuations\n",
    "    generated_clean = '. '.join(sentence.strip().capitalize() for sentence in generated.split('. '))\n",
    "    \n",
    "    return generated_clean\n",
    "\n",
    "N=5\n",
    "\n",
    "SEP=\" \"\n",
    "\n",
    "sentence_count=5\n",
    "\n",
    "ngrams = build_ngrams()\n",
    "start_seq=\"We have\"\n",
    "\n",
    "counts = ngram_freqs(ngrams)\n",
    "\n",
    "if start_seq is None or start_seq not in counts: start_seq = random.choice(list(counts.keys()))\n",
    "generated = start_seq.lower();\n",
    "\n",
    "sentences = 0\n",
    "while sentences < sentence_count:\n",
    "    next_word_result = next_word(generated, N, counts)\n",
    "    if next_word_result is not None:\n",
    "        generated += SEP + next_word_result\n",
    "        sentences += 1 if generated.endswith(('.','!', '?')) else 0\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difficult or impossible if a person lacks adequate shelter. The interactions between health and homelessness are complex defying sweeping generalizations as to cause or effect. If we look to the future preventing homelessness is an important objective. This will require us to develop a much more sophisticated understanding of the dynamics of homelessness than we currently possess an understanding that can be developed only through careful study and research. William r.\n"
     ]
    }
   ],
   "source": [
    "print(clean_generated(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
